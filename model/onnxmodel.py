import numpy as np
import onnx
import onnxruntime as ort


# --------------------- 1. æ£€æŸ¥ONNXæ¨¡å‹ ---------------------
def check_onnx_model(model_path):
    """éªŒè¯ONNXæ¨¡å‹æ ¼å¼æ˜¯å¦æ­£ç¡®"""
    try:
        model = onnx.load(model_path)
        onnx.checker.check_model(model)
        print("âœ… ONNX æ¨¡å‹æ£€æŸ¥é€šè¿‡")

        # æ‰“å°æ¨¡å‹è¾“å…¥/è¾“å‡ºä¿¡æ¯
        print("\nğŸ“Œ æ¨¡å‹è¾“å…¥è¾“å‡ºä¿¡æ¯:")
        for input in model.graph.input:
            print(f"  è¾“å…¥åç§°: {input.name}, å½¢çŠ¶: {input.type.tensor_type.shape}")
        for output in model.graph.output:
            print(f"  è¾“å‡ºåç§°: {output.name}, å½¢çŠ¶: {output.type.tensor_type.shape}")

        return model
    except Exception as e:
        print(f"âŒ ONNX æ¨¡å‹æ£€æŸ¥å¤±è´¥: {e}")
        raise


# --------------------- 2. æ•°æ®é¢„å¤„ç† ---------------------



# --------------------- 3. ONNXæ¨ç† ---------------------
def run_onnx_inference(model_path, input_data):
    """æ‰§è¡ŒONNXæ¨ç†"""
    try:
        # åˆ›å»ºæ¨ç†ä¼šè¯ï¼ˆå¯é€‰GPU/CPUï¼‰
        providers = ["CPUExecutionProvider"]  # ä¼˜å…ˆä½¿ç”¨GPU
        sess = ort.InferenceSession(model_path, providers=providers)

        # è·å–è¾“å…¥åç§°
        input_name = sess.get_inputs()[0].name
        # æ‰“å° ONNX æ¨¡å‹çš„è¾“å…¥ä¿¡æ¯
        for input in sess.get_inputs():
            print(f"Input name: {input.name}, Shape: {input.shape}, Type: {input.type}")
        # å¯¹æ¯ä¸€å±‚æ‰§è¡Œæ¨ç†å¹¶æ‰“å°è¾“å‡º
        for layer in sess.get_outputs():
            layer_output = sess.run([layer.name], {input_name: input_data})
            print(f"Output name: {layer.name}, Output shape: {layer_output[0].shape}")
        # è¿è¡Œæ¨ç†
        outputs = sess.run(None, {input_name: input_data})

        print(f"ğŸ¯ æ¨ç†å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {outputs[0].shape}")
        return outputs
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        raise


# --------------------- 4. ç»“æœåå¤„ç† ---------------------
def postprocess_output(output):
    """åå¤„ç†è¾“å‡ºç»“æœï¼ˆç¤ºä¾‹ï¼šåˆ†ç±»ä»»åŠ¡å–Top-5ï¼‰"""
    probs = np.squeeze(output)  # å»é™¤batchç»´åº¦
    top_indices = np.argsort(probs)[-5:][::-1]  # å–æ¦‚ç‡æœ€é«˜çš„5ä¸ªç±»åˆ«

    # ç¤ºä¾‹ï¼šå‡è®¾æ˜¯ImageNetç±»åˆ«ï¼ˆå®é™…éœ€æ›¿æ¢ä¸ºä½ çš„ç±»åˆ«æ ‡ç­¾ï¼‰
    imagenet_labels = [...]  # è¿™é‡Œåº”æ›¿æ¢ä¸ºå®é™…çš„ç±»åˆ«æ ‡ç­¾åˆ—è¡¨
    print("\nğŸ† æ¨ç†ç»“æœ:")
    for i, idx in enumerate(top_indices):
        print(f"  {i + 1}. {imagenet_labels[idx]} (æ¦‚ç‡: {probs[idx]:.4f})")

    return top_indices

class net:
    def __init__(self,model_path,feature_num):
        self.net=check_onnx_model(model_path)
        self.model_path=model_path
        if feature_num==4:
            self.label=['å¥åº·','è½»åº¦æŠ‘éƒ','ä¸­åº¦æŠ‘éƒ','é‡åº¦æŠ‘éƒ']
        else:
            self.label=['å¥åº·','æŠ‘éƒ']
        # providers=["CUDAExecutionProvider"]
        self.sess = ort.InferenceSession(model_path)
        self.input_name = self.sess.get_inputs()[0].name
    def predict(self,x):
        try:
            x = np.reshape(x,(1,1,64,500))
        except Exception as e:
            print(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")

            return np.array([0.5,0.5])
        y = self.sess.run(None, {self.input_name: x})
        # y=run_onnx_inference(self.model_path, x)
        y = y[0][0]
        EEG_doctor = {}
        for i in range(len(y)):
            EEG_doctor[self.label[i]] = y[i]
        return EEG_doctor
# --------------------- ä¸»ç¨‹åº ---------------------
if __name__ == "__main__":
    # é…ç½®è·¯å¾„å’Œå‚æ•°

    net1=net('model_cuda.onnx',4)

    x=np.random.randn(1,1,64,500).astype(np.float32)
    print(net1.predict(x))
    exit()

    ONNX_MODEL_PATH = "model_cuda.onnx"  # æ›¿æ¢ä¸ºä½ çš„ONNXæ¨¡å‹è·¯å¾„
    IMAGE_PATH = "test.jpg"  # æ›¿æ¢ä¸ºæµ‹è¯•å›¾åƒè·¯å¾„
    INPUT_SHAPE = [1,1, 64, 500]  # æ¨¡å‹è¾“å…¥å½¢çŠ¶ [batch, channel, height, width]

    try:
        # 1. æ£€æŸ¥æ¨¡å‹
        model = check_onnx_model(ONNX_MODEL_PATH)

        # 2. æ•°æ®é¢„å¤„ç†
        input_data = np.random.randn(3, 1,64,500).astype(np.float32)

        # 3. æ‰§è¡Œæ¨ç†
        outputs = run_onnx_inference(ONNX_MODEL_PATH, input_data)

        # 4. åå¤„ç†ç»“æœ
        postprocess_output(outputs[0])

    except Exception as e:
        print(f"ğŸ”´ ä¸»ç¨‹åºå‡ºé”™: {e}")